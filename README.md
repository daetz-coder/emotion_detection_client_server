# å®æ—¶é¢éƒ¨æƒ…ç»ªè¯†åˆ«

è¯´æ˜ï¼šæœ¬é¡¹ç›®é‡ç‚¹åœ¨äºæƒ…ç»ªè¯†åˆ«ï¼Œè‡³äºäººè„¸æ£€æµ‹ä½¿ç”¨opencvå†…ç½®çš„ **Haar ç‰¹å¾çº§è”åˆ†ç±»å™¨**ï¼ˆHaar Cascade Classifierï¼‰æ¥è¿›è¡Œäººè„¸æ£€æµ‹



>å®Œæ•´çš„ä»£ç è§ï¼š[daetz-coder/emotion_detection_client_server: Real-Time Facial EmotionDetection With Server and Local (github.com)](https://github.com/daetz-coder/emotion_detection_client_server) æˆ–è€…[emotion_detection_client_server: Real-Time Facial EmotionDetection (gitee.com)](https://gitee.com/daetz_0/emotion_detection_client_server)
>
>æƒé‡å’Œæ•°æ®é›†è§ï¼š[Release v1.0 Â· daetz-coder/emotion_detection_client_server (github.com)](https://github.com/daetz-coder/emotion_detection_client_server/releases/tag/model-weight-dataset-upload)
>
>æ¡†æ¶éƒ¨åˆ†å‚è€ƒäº†[serengil/deepface: A Lightweight Face Recognition and Facial Attribute Analysis (Age, Gender, Emotion and Race) Library for Python (github.com)](https://github.com/serengil/deepface)



## ç®€ä»‹

æœ¬é¡¹ç›®æ—¨åœ¨å®ç°å®æ—¶é¢éƒ¨æƒ…ç»ªè¯†åˆ«ã€‚å…¶æ ¸å¿ƒæµç¨‹é¦–å…ˆä½¿ç”¨ OpenCV å†…ç½®çš„ Haar çº§è”åˆ†ç±»å™¨è¿›è¡Œäººè„¸æ£€æµ‹ï¼Œå†é€šè¿‡å¤šç§å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹å¯¹æå–çš„äººè„¸è¿›è¡Œæƒ…ç»ªåˆ†ç±»è¯†åˆ«ã€‚é¡¹ç›®æä¾›äº†å¤šç§æ¨¡å‹æ¶æ„é€‰æ‹©ï¼ŒåŒ…æ‹¬åŸºäºç°åº¦å›¾ï¼ˆ48Ã—48ï¼‰ã€RGB å›¾åƒï¼ˆ96Ã—96ã€224Ã—224ï¼‰çš„è‡ªå®šä¹‰å·ç§¯ç½‘ç»œï¼Œä»¥åŠåŸºäºé¢„è®­ç»ƒçš„ EfficientNetV2 æ¨¡å‹ï¼Œå¹¶åœ¨ä¸ƒç§æƒ…ç»ªï¼ˆå¦‚å¿«ä¹ã€æ„¤æ€’ã€æ‚²ä¼¤ã€æƒŠè®¶ç­‰ï¼‰ä¸Šè¿›è¡Œäº†è®­ç»ƒå’Œå¾®è°ƒã€‚ä¸ºå¼¥è¡¥ç°æœ‰æ•°æ®é›†ä¸­äºšæ´²äººè„¸æ•°æ®è¾ƒå°‘çš„é—®é¢˜ï¼Œé¡¹ç›®å›¢é˜Ÿè¿˜è‡ªè¡Œæ”¶é›†äº†åŒ…å«äºšæ´²äººé¢éƒ¨è¡¨æƒ…çš„æ•°æ®é›†ç”¨äºè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒæ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€å‚æ•°è§„æ¨¡å’Œè¿ç®—å¤æ‚åº¦ï¼Œç”¨æˆ·å¯æ ¹æ®å®æ—¶æ€§è¦æ±‚é€‰ç”¨åˆé€‚çš„æ¨¡å‹ã€‚åŒæ—¶ï¼Œé¡¹ç›®å¼€æ”¾äº†å®Œæ•´çš„ä»£ç ã€è®­ç»ƒè®°å½•ã€æ¨¡å‹æƒé‡åŠä½¿ç”¨è¯´æ˜ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿæ–¹ä¾¿åœ°åœ¨æœ¬åœ°æˆ–æœåŠ¡å™¨ç¯å¢ƒä¸­éƒ¨ç½²å’Œåº”ç”¨è¿™ä¸€å®æ—¶æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿã€‚å°½ç®¡è¯¥å®æ—¶é¢éƒ¨æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿåœ¨æŠ€æœ¯ä¸Šå–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†ä»å­˜åœ¨ä¸€äº›ä¸è¶³ä¹‹å¤„ã€‚é¦–å…ˆï¼Œç”±äºæƒ…ç»ªæ•°æ®é›†çš„æ ‡æ³¨å·¥ä½œéš¾åº¦è¾ƒå¤§ï¼Œç°æœ‰æ•°æ®é›†çš„è´¨é‡å’Œå¤šæ ·æ€§æœ‰é™ï¼Œè¿™å¯¹æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå‡†ç¡®æ€§é€ æˆäº†åˆ¶çº¦ã€‚å…¶æ¬¡ï¼Œæ¨¡å‹æ¶æ„é€‰æ‹©ã€å‚æ•°è®¾ç½®ç­‰å› ç´ ä¹Ÿå½±å“äº†æœ€ç»ˆçš„è¯†åˆ«æ€§èƒ½ï¼Œéƒ¨åˆ†æ¨¡å‹åœ¨ç‰¹å®šæƒ…ç»ªçš„åŒºåˆ†ä¸Šè¡¨ç°ä¸è¶³ï¼Œå¹¶å­˜åœ¨è¿‡æ‹Ÿåˆç°è±¡ã€‚é¢å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœªæ¥çš„å·¥ä½œå°†é›†ä¸­äºï¼šæ‰©å¤§å’Œä¼˜åŒ–æ ‡æ³¨æ•°æ®é›†ï¼Œç‰¹åˆ«æ˜¯å¼•å…¥æ›´å¤šäºšæ´²é¢å­”åŠå¤šæ ·åŒ–è¡¨æƒ…çš„æ•°æ®ä»¥ä¸°å¯Œè®­ç»ƒæ ·æœ¬ï¼›æ¢ç´¢æ›´å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å’Œä¼˜åŒ–ç®—æ³•ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œç²¾ç¡®åº¦ï¼›ä»¥åŠåœ¨æ¨¡å‹å‹ç¼©ã€åŠ é€Ÿæ¨ç†ç­‰æ–¹é¢å¼€å±•ç ”ç©¶ï¼Œä»¥æ»¡è¶³å®æ—¶æ£€æµ‹å¯¹äºé€Ÿåº¦ä¸å‡†ç¡®ç‡çš„åŒé‡è¦æ±‚ã€‚è¿™äº›æ”¹è¿›å°†æœ‰åŠ©äºè¿›ä¸€æ­¥æå‡ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚



## ä¸€ã€æ•°æ®é›†

### 1ã€é¢„è®­ç»ƒæ•°æ®

[Facial Emotion Recognition Image Dataset (kaggle.com)](https://www.kaggle.com/datasets/sujaykapadnis/emotion-recognition-dataset)

è¯¥æ•°æ®é›†åŒ…å« 6 ç§ä¸åŒçš„æƒ…ç»ªï¼šå¿«ä¹ã€æ„¤æ€’ã€æ‚²ä¼¤ã€ä¸­ç«‹ã€æƒŠè®¶ç­‰è¡¨æƒ…ã€‚è¯¥æ•°æ®é›†æ˜¯é€šè¿‡æŠ“å– Facebook å’Œ Instagram ç­‰ç¤¾äº¤ç½‘ç»œã€æŠ“å– YouTube è§†é¢‘å’Œå·²æœ‰çš„ IMDB å’Œ AffectNet æ•°æ®é›†æ”¶é›†çš„ã€‚



### 2ã€æ”¶é›†çš„æ•°æ®é›†

ç”±äºç½‘ä¸Šå¸¸è§çš„æ•°æ®é›†å¤§éƒ¨åˆ†çš„éƒ½æ˜¯æ¬§æ´²çš„äººè„¸ï¼Œå¯¹äºšæ´²ï¼Œå›½äººçš„äººè„¸æ¯”è¾ƒå°‘ï¼Œè¿™é‡Œæˆ‘ä»¬è‡ªå·±è¿›è¡Œæ”¶é›†ï¼Œç”¨äºå¾®è°ƒï¼Œæˆ‘ä»¬ä»å½±è§†å‰§ã€äº’è”ç½‘ä¸Šæ”¶é›†äº†ä¸€ä¸ªåŒ…å«ä¸ƒç§æƒ…æ„Ÿçš„äººè„¸æ•°æ®é›†ï¼ˆäºšæ´²ã€å›½äººï¼‰ï¼Œç”¨äºå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç”±äºéƒ¨åˆ†éƒ¨åˆ†è¡¨æƒ…ä¹‹é—´éš¾ä»¥åŒºåˆ†ä¸”æ˜¯äººå·¥æ ‡æ³¨ï¼Œæ•°æ®é›†è´¨é‡æœ‰å¾…æå‡ï¼Œæœ€ç»ˆåŒ…å«å¦‚ä¸‹å‡ ç§ç±»å‹ï¼Œä¸€å…±æœ‰882å¼ æ•°æ®ï¼Œç±»å‹åˆ†å¸ƒå¦‚ä¸‹ï¼š

<img src="https://daetz-image.oss-cn-hangzhou.aliyuncs.com/img/202412232156895.png" alt="image-20241223215644766" style="zoom: 33%;" />





éƒ¨åˆ†æ•°æ®é›†å±•ç¤ºå¦‚ä¸‹

![image-20241223220034836](https://daetz-image.oss-cn-hangzhou.aliyuncs.com/img/202412232200324.png)

å®Œæ•´çš„æ•°æ®é›†å¯è®¿é—®ï¼š

+ [emotion_detection_client_server_release](https://gitee.com/daetz_0/emotion_detection_client_server/releases)

+ https://gitee.com/daetz_0/emotion_detection_client_server/releases/download/model-weight-data-upload/emotion_dataset.zip



## äºŒã€æ¨¡å‹



### 1ã€å·ç§¯æ¨¡å‹ï¼ˆGRAY+size48ï¼‰



æ¨¡å‹æ¶æ„å¦‚ä¸‹,ä½¿ç”¨ç°åº¦å›¾ ï¼Œå¹¶ä¸”å›¾åƒçš„å°ºå¯¸æ˜¯48

```python
# å®šä¹‰æ¨¡å‹æ¶æ„
model = Sequential()
input_shape = (48, 48, 1)  # è¾“å…¥å›¾åƒå°ºå¯¸

# 1st convolution layer
model.add(Conv2D(32, (3,3), activation="selu", input_shape=input_shape, padding='same'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.3))

# 2nd convolution layer
model.add(Conv2D(64, (3,3), activation="selu", padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3,3), activation="selu", padding='same'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.4))

# 3rd convolution layer
model.add(Conv2D(128, (3,3), activation="selu", padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3,3), activation="selu", padding='same'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.5))

# 4th convolution layer
model.add(Conv2D(256, (3,3), activation="selu", padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(256, (3,3), activation="selu", padding='same'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.6))

# Flatten layer
model.add(Flatten())

# Fully connected neural networks
model.add(Dense(128, activation='selu', kernel_regularizer=l2(0.01)))
model.add(BatchNormalization())
model.add(Dropout(0.5))

# Output layer with softmax activation
model.add(Dense(7, activation='softmax'))

# ç¼–è¯‘æ¨¡å‹
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# æ‰“å°æ¨¡å‹æ‘˜è¦
model.summary()

```





### 2ã€å·ç§¯æ¨¡å‹ï¼ˆRGB+size96ï¼‰



```python
model_4 = Sequential()

model_4.add(Conv2D(32, (3,3), activation="selu", input_shape=input_shape))
model_4.add(BatchNormalization())
model_4.add(MaxPool2D(pool_size=(2,2)))
model_4.add(Dropout(0.3))

model_4.add(Conv2D(64, (3,3), activation="selu"))
model_4.add(BatchNormalization())
model_4.add(Conv2D(64, (3,3), activation="selu"))
model_4.add(BatchNormalization())
model_4.add(MaxPool2D(pool_size=(2,2)))
model_4.add(Dropout(0.4))

model_4.add(Conv2D(128, (3,3), activation="selu"))
model_4.add(BatchNormalization())
model_4.add(Conv2D(128, (3,3), activation="selu"))
model_4.add(BatchNormalization())
model_4.add(MaxPool2D(pool_size=(2,2)))
model_4.add(Dropout(0.5))

model_4.add(Conv2D(256, (3,3), activation="selu"))
model_4.add(BatchNormalization())
model_4.add(Conv2D(256, (3,3), activation="selu"))
model_4.add(BatchNormalization())
model_4.add(MaxPool2D(pool_size=(2,2)))
model_4.add(Dropout(0.6))

model_4.add(Flatten())
model_4.add(Dense(128, activation='selu', kernel_regularizer=l2(0.01)))
model_4.add(BatchNormalization())
model_4.add(Dropout(0.5))
model_4.add(Dense(7, activation='softmax'))

model_4.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

model_4.summary()
```



### 3ã€å·ç§¯æ¨¡å‹ï¼ˆRGB+size224ï¼‰

```python
# å®šä¹‰æ¨¡å‹æ¶æ„
model_4 = Sequential()
num_classes = 7
input_shape = (224, 224, 3)  # è¾“å…¥å›¾åƒå°ºå¯¸ä¸º224x224ï¼ŒRGBä¸‰é€šé“

# ç¬¬ä¸€ç»„å·ç§¯å±‚
model_4.add(Conv2D(32, (3,3), activation="selu", input_shape=input_shape, padding='same'))
model_4.add(BatchNormalization())
model_4.add(MaxPool2D(pool_size=(2,2)))
model_4.add(Dropout(0.3))

# ç¬¬äºŒç»„å·ç§¯å±‚
model_4.add(Conv2D(64, (3,3), activation="selu", padding='same'))
model_4.add(BatchNormalization())
model_4.add(Conv2D(64, (3,3), activation="selu", padding='same'))
model_4.add(BatchNormalization())
model_4.add(MaxPool2D(pool_size=(2,2)))
model_4.add(Dropout(0.4))

# ç¬¬ä¸‰ç»„å·ç§¯å±‚
model_4.add(Conv2D(128, (3,3), activation="selu", padding='same'))
model_4.add(BatchNormalization())
model_4.add(Conv2D(128, (3,3), activation="selu", padding='same'))
model_4.add(BatchNormalization())
model_4.add(MaxPool2D(pool_size=(2,2)))
model_4.add(Dropout(0.5))

# ç¬¬å››ç»„å·ç§¯å±‚
model_4.add(Conv2D(256, (3,3), activation="selu", padding='same'))
model_4.add(BatchNormalization())
model_4.add(Conv2D(256, (3,3), activation="selu", padding='same'))
model_4.add(BatchNormalization())
model_4.add(MaxPool2D(pool_size=(2,2)))
model_4.add(Dropout(0.6))

# ç¬¬äº”ç»„å·ç§¯å±‚ï¼ˆå¯é€‰ï¼Œè¿›ä¸€æ­¥å¢åŠ å¤æ‚æ€§ï¼‰
model_4.add(Conv2D(512, (3,3), activation="selu", padding='same'))
model_4.add(BatchNormalization())
model_4.add(Conv2D(512, (3,3), activation="selu", padding='same'))
model_4.add(BatchNormalization())
model_4.add(MaxPool2D(pool_size=(2,2)))
model_4.add(Dropout(0.7))

# Flattenå±‚
model_4.add(Flatten())

# å…¨è¿æ¥å±‚
model_4.add(Dense(512, activation='selu', kernel_regularizer=l2(0.01)))
model_4.add(BatchNormalization())
model_4.add(Dropout(0.5))

model_4.add(Dense(256, activation='selu', kernel_regularizer=l2(0.01)))
model_4.add(BatchNormalization())
model_4.add(Dropout(0.5))

# è¾“å‡ºå±‚
model_4.add(Dense(num_classes, activation="softmax"))

# ç¼–è¯‘æ¨¡å‹
model_4.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# æ‰“å°æ¨¡å‹æ‘˜è¦
model_4.summary()

```



### 4ã€EffectiveV2æ¨¡å‹ï¼ˆRGB+size224ï¼‰

è¿™é‡Œä½¿ç”¨äº†imagenetçš„æƒé‡ï¼Œæˆ‘ä»¬è¿›åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒ

è¿™é‡Œè¿˜å¯ä»¥ä½¿ç”¨Facial Emotion Recognition Image Dataset ç”¨äºåœ¨äººè„¸æƒ…ç»ªæ–¹é¢è¿›è¡Œè®­ç»ƒï¼Œä¿ç•™æƒé‡åå†è‡ªå·±æ”¶é›†çš„æ•°æ®é›†ä¸Šå¾®è°ƒ(å¯é€‰)

```python
# Import the EfficientNetV2M model pre-trained on ImageNet without the top layers
base_model = tf.keras.applications.ConvNeXtTiny(input_shape=(224, 224, 3), include_top=False, weights='imagenet')

# Freeze the base model layers to prevent them from being trained
base_model.trainable = True  #Unfreeze Some Layers for Fine-Tuning

fine_tune_at = len(base_model.layers) - 20  
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

# Create  model
keras_model = keras.models.Sequential()
keras_model.add(base_model)
keras_model.add(keras.layers.GlobalAveragePooling2D())  # Replace Flatten with GlobalAveragePooling
keras_model.add(keras.layers.BatchNormalization())  # Add a Batch Normalization Layer
keras_model.add(keras.layers.Dense(128, activation='relu'))  # Increase the Model Capacity
keras_model.add(keras.layers.Dropout(0.3))  # Reduce Regularization
keras_model.add(keras.layers.Dense(7, activation=tf.nn.softmax))  # 6 output units for classification

# Display the model's architecture
keras_model.summary()

```



### 5ã€æ•°æ®åŠ è½½åŒºåˆ«

![image-20241223221258667](https://daetz-image.oss-cn-hangzhou.aliyuncs.com/img/202412232212800.png)



**å®Œæ•´çš„ä»£ç å’Œè¿è¡Œè®°å½•å¯è®¿é—®**

[emotion_detection_client_server: Real-Time Facial EmotionDetection - Gitee.com](https://gitee.com/daetz_0/emotion_detection_client_server/tree/main/Tutorial/model)

æˆ–è€…[daetz-coder/emotion_detection_client_server: Real-Time Facial EmotionDetection With Server and Local (github.com)](https://github.com/daetz-coder/emotion_detection_client_server)

+ best_model_CNN_GRAY_size48.ipynb
+ best_model_CNN_RGB_size96
+ best_model_CNN_RGB_size224
+ best_model_EffectiveV2_RGB_size224



## ä¸‰ã€ç»“æœ



### 1ã€æ— é¢„è®­ç»ƒ

åœ¨è¿™é‡Œä¸ä½¿ç”¨ä»»ä½•é¢„è®­ç»ƒçš„æ•°æ®é›†æˆ–è€…æ¨¡å‹ç»“æ„ï¼Œä»…ä»…ä½¿ç”¨è‡ªå·±æ”¶é›†çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œå¾®è°ƒ

#### 1)ã€å·ç§¯æ¨¡å‹ï¼ˆGRAY+size48ï¼‰

```bash
Epoch 40/100
23/23 [==============================] - ETA: 0s - loss: 2.9194 - accuracy: 0.4553
Epoch 40: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
23/23 [==============================] - 1s 23ms/step - loss: 2.9194 - accuracy: 0.4553 - val_loss: 3.2149 - val_accuracy: 0.3722 - lr: 3.1250e-05
Epoch 41/100
23/23 [==============================] - 1s 23ms/step - loss: 2.9375 - accuracy: 0.4413 - val_loss: 3.2338 - val_accuracy: 0.3667 - lr: 1.5625e-05
Epoch 42/100
22/23 [===========================>..] - ETA: 0s - loss: 2.8755 - accuracy: 0.4659
Epoch 42: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.
23/23 [==============================] - 1s 24ms/step - loss: 2.8805 - accuracy: 0.4637 - val_loss: 3.2164 - val_accuracy: 0.3833 - lr: 1.5625e-05
Epoch 43/100
23/23 [==============================] - 1s 24ms/step - loss: 2.8275 - accuracy: 0.4846 - val_loss: 3.2361 - val_accuracy: 0.3667 - lr: 7.8125e-06
Epoch 44/100
22/23 [===========================>..] - ETA: 0s - loss: 2.7978 - accuracy: 0.4787
Epoch 44: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.
23/23 [==============================] - 1s 24ms/step - loss: 2.7997 - accuracy: 0.4777 - val_loss: 3.2567 - val_accuracy: 0.3611 - lr: 7.8125e-06
```



![image-20241223222511357](https://daetz-image.oss-cn-hangzhou.aliyuncs.com/img/202412232225416.png)



#### 2ã€å·ç§¯æ¨¡å‹ï¼ˆRGB+size96ï¼‰



```bash
Epoch 42/3000
23/23 [==============================] - 1s 25ms/step - loss: 2.1134 - accuracy: 0.6006 - val_loss: 2.4665 - val_accuracy: 0.5000 - lr: 3.9063e-06
Epoch 43/3000
22/23 [===========================>..] - ETA: 0s - loss: 2.1313 - accuracy: 0.5824
Epoch 43: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.
23/23 [==============================] - 1s 24ms/step - loss: 2.1382 - accuracy: 0.5810 - val_loss: 2.4613 - val_accuracy: 0.5056 - lr: 3.9063e-06
Epoch 44/3000
23/23 [==============================] - 1s 24ms/step - loss: 2.1184 - accuracy: 0.5950 - val_loss: 2.4612 - val_accuracy: 0.5056 - lr: 1.9531e-06
Epoch 45/3000
22/23 [===========================>..] - ETA: 0s - loss: 2.1409 - accuracy: 0.5597
Epoch 45: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.
23/23 [==============================] - 1s 23ms/step - loss: 2.1338 - accuracy: 0.5615 - val_loss: 2.4612 - val_accuracy: 0.5056 - lr: 1.9531e-06
```

![image-20241223222630714](https://daetz-image.oss-cn-hangzhou.aliyuncs.com/img/202412232226780.png)



#### 3ã€å·ç§¯æ¨¡å‹ï¼ˆRGB+size224ï¼‰

```bash
Epoch 90/100
23/23 [==============================] - 3s 111ms/step - loss: 2.2417 - accuracy: 0.9749 - val_loss: 4.3400 - val_accuracy: 0.5111 - lr: 3.9063e-06
Epoch 91/100
22/23 [===========================>..] - ETA: 0s - loss: 2.2400 - accuracy: 0.9759
Epoch 91: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.
23/23 [==============================] - 3s 112ms/step - loss: 2.2397 - accuracy: 0.9763 - val_loss: 4.3405 - val_accuracy: 0.5056 - lr: 3.9063e-06
Epoch 92/100
23/23 [==============================] - 3s 113ms/step - loss: 2.2288 - accuracy: 0.9874 - val_loss: 4.3479 - val_accuracy: 0.5056 - lr: 1.9531e-06
Epoch 93/100
23/23 [==============================] - ETA: 0s - loss: 2.2212 - accuracy: 0.9846
Epoch 93: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.
23/23 [==============================] - 3s 115ms/step - loss: 2.2212 - accuracy: 0.9846 - val_loss: 4.3524 - val_accuracy: 0.5056 - lr: 1.9531e-06
Epoch 94/100
23/23 [==============================] - 3s 114ms/step - loss: 2.2349 - accuracy: 0.9791 - val_loss: 4.3532 - val_accuracy: 0.5056 - lr: 9.7656e-07
Epoch 95/100
23/23 [==============================] - ETA: 0s - loss: 2.2182 - accuracy: 0.9832
Epoch 95: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.
23/23 [==============================] - 3s 117ms/step - loss: 2.2182 - accuracy: 0.9832 - val_loss: 4.3539 - val_accuracy: 0.5056 - lr: 9.7656e-07
```

![image-20241223222732866](https://daetz-image.oss-cn-hangzhou.aliyuncs.com/img/202412232227934.png)



### 2ã€å«é¢„è®­ç»ƒ

#### 4) EffectiveV2æ¨¡å‹ï¼ˆRGB+size224ï¼‰

![image-20241223222900241](https://daetz-image.oss-cn-hangzhou.aliyuncs.com/img/202412232229595.png)

![image-20241223222833220](https://daetz-image.oss-cn-hangzhou.aliyuncs.com/img/202412232228306.png)



è™½ç„¶è¡¨ç°çš„æ¯”ä¹‹å‰å¥½ï¼Œä½†æ˜¯å‡ºç°äº†æ˜æ˜¾çš„è¿‡æ‹Ÿåˆçš„é—®é¢˜



### 3ã€å‚æ•°æ¯”è¾ƒ

å››ç§æ¨¡å‹çš„å®Œæ•´æƒé‡å¯è®¿é—®ï¼Œæƒé‡æ–‡ä»¶éœ€è¦æ”¾å…¥`pth`æ–‡ä»¶å¤¹ä¸‹

+ https://gitee.com/daetz_0/emotion_detection_client_server/releases

+ [ best_model_CNN_Gray_size48.h5 ](https://gitee.com/daetz_0/emotion_detection_client_server/releases/download/model-weight-data-upload/best_model_CNN_Gray_size48.h5)

+ [ best_model_CNN_RGB_size96.h5 ](https://gitee.com/daetz_0/emotion_detection_client_server/releases/download/model-weight-data-upload/best_model_CNN_RGB_size96.h5)

+ https://pan.baidu.com/s/1TgOv-Eeojh62JPSAx6AeWw?pwd=2024

ä»å®éªŒç»“æœæ¥çœ‹ï¼Œå‚æ•°è¶Šå¤šæ¨¡å‹çš„æ£€æµ‹æ€§èƒ½è¶Šå¥½ï¼Œä½†æ˜¯éšä¹‹è€Œæ¥çš„æ˜¯å¤æ‚çš„è®¡ç®—é‡ï¼Œå‚æ•°é‡æˆå€æå‡ï¼Œç”±äºå®æ—¶æ£€æµ‹å¯¹å»¶æ—¶çš„è¦æ±‚è¾ƒé«˜ï¼Œéœ€è¦è‡ªè¡Œæƒè¡¡

![image-20241223223409956](https://daetz-image.oss-cn-hangzhou.aliyuncs.com/img/202412232234037.png)





## å››ã€ä½¿ç”¨

### 1ã€ç”¨äºè®­ç»ƒ

[emotion_detection_client_server: Real-Time Facial EmotionDetection - Gitee.com](https://gitee.com/daetz_0/emotion_detection_client_server/tree/main/Tutorial/model)

+ best_model_CNN_GRAY_size48.ipynb
+ best_model_CNN_RGB_size96
+ best_model_CNN_RGB_size224
+ best_model_EffectiveV2_RGB_size224

ä»…éœ€è¦ä½¿ç”¨ä¸Šè¿°çš„å†…å®¹ï¼Œå»ºè®® tensorflow>=2.9.0 Python 3.8(ubuntu20.04) Cuda 11.2

```less
drwxr-xr-x  4 root root   46 Dec 23 22:41 ./
drwxr-xr-x 13 root root 4096 Dec 23 22:39 ../
drwxr-xr-x  9 root root  133 Dec 23 22:39 dataset/
drwxr-xr-x  2 root root 4096 Dec 23 22:40 model/
-rw-r--r-- 1 root root  187223 Dec 23 22:40 best_model_CNN_GRAY_size48.ipynb
-rw-r--r-- 1 root root  270266 Dec 23 22:40 best_model_CNN_RGB_size224.ipynb
-rw-r--r-- 1 root root  260946 Dec 23 22:40 best_model_CNN_RGB_size96.ipynb
-rw-r--r-- 1 root root 1029572 Dec 23 22:40 best_model_EffectiveV2_RGB_size224.ipynb
```



### 2ã€ç”¨äºæ£€æµ‹

å¯¹äºä¸Šè¿°çš„å››ç§æ–¹æ³•ï¼Œæˆ‘ä»¬åˆ†åˆ«æä¾›å››ç§ä¸åŒçš„åŠ è½½æ–‡ä»¶ï¼Œåˆ†åˆ«ä½¿ç”¨ä¸‹è¿°å†…å®¹ï¼Œä¸Šä¼ ä¸åŒæ ¼å¼çš„å›¾åƒ

+ cv2.COLOR_BGR2GRAY
+ cv2.COLOR_BGR2RGB
+ cv2.resize(img_gray, (48, 48))
+ cv2.resize(image_rgb, (96, 96))
+ cv2.resize(image_rgb, (224, 224))

å…³é”®éƒ¨åˆ†ä»£ç å¦‚ä¸‹,å®Œæ•´çš„å†…å®¹å¯è®¿é—®`EmotionDetection/modules`

```python
class EmotionClient(Demography):
    """
    æƒ…ç»ªè¯†åˆ«æ¨¡å‹ç±»
    """

    def __init__(self):
        # åŠ è½½æ¨¡å‹
        self.model = load_model()
        self.model_name = "Emotion"

    def predict(self, img: np.ndarray) -> np.ndarray:
        image_rgb = cv2.cvtColor(img[0], cv2.COLOR_BGR2RGB)
        image_resized = cv2.resize(image_rgb, (224, 224))
        # print(f"image_resized: {image_resized.shape}")
        # å½’ä¸€åŒ–å›¾åƒæ•°æ®ï¼ˆå‡è®¾æ¨¡å‹åœ¨ 0-1 èŒƒå›´å†…è®­ç»ƒï¼‰
        image = np.expand_dims(image_resized, axis=0)
        # print(f"image_batch: {image_batch.shape}")
        # image_batch = image_batch.astype('float32') / 255.0

        # è¿›è¡Œé¢„æµ‹ï¼Œé¿å…ä½¿ç”¨ `model.predict` ä»¥å‡å°‘å†…å­˜é—®é¢˜
        emotion_predictions = self.model(image, training=False).numpy()[0, :]

        return emotion_predictions


def load_model() -> Sequential:
    """
    æ„å»ºæƒ…ç»ªè¯†åˆ«æ¨¡å‹ï¼ŒåŠ è½½æœ¬åœ°æƒé‡æ–‡ä»¶
    """
    model = Sequential()
    num_classes = 7
    input_shape = (224, 224, 3)  # è¾“å…¥å›¾åƒå°ºå¯¸ä¸º224x224ï¼ŒRGBä¸‰é€šé“

    # ç¬¬ä¸€ç»„å·ç§¯å±‚
    model.add(Conv2D(32, (3,3), activation="selu", input_shape=input_shape, padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPool2D(pool_size=(2,2)))
    model.add(Dropout(0.3))

    # ç¬¬äºŒç»„å·ç§¯å±‚
    model.add(Conv2D(64, (3,3), activation="selu", padding='same'))
    model.add(BatchNormalization())
    model.add(Conv2D(64, (3,3), activation="selu", padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPool2D(pool_size=(2,2)))
    model.add(Dropout(0.4))

    # ç¬¬ä¸‰ç»„å·ç§¯å±‚
    model.add(Conv2D(128, (3,3), activation="selu", padding='same'))
    model.add(BatchNormalization())
    model.add(Conv2D(128, (3,3), activation="selu", padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPool2D(pool_size=(2,2)))
    model.add(Dropout(0.5))

    # ç¬¬å››ç»„å·ç§¯å±‚
    model.add(Conv2D(256, (3,3), activation="selu", padding='same'))
    model.add(BatchNormalization())
    model.add(Conv2D(256, (3,3), activation="selu", padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPool2D(pool_size=(2,2)))
    model.add(Dropout(0.6))

    # ç¬¬äº”ç»„å·ç§¯å±‚ï¼ˆå¯é€‰ï¼Œè¿›ä¸€æ­¥å¢åŠ å¤æ‚æ€§ï¼‰
    model.add(Conv2D(512, (3,3), activation="selu", padding='same'))
    model.add(BatchNormalization())
    model.add(Conv2D(512, (3,3), activation="selu", padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPool2D(pool_size=(2,2)))
    model.add(Dropout(0.7))

    # Flattenå±‚
    model.add(Flatten())

    # å…¨è¿æ¥å±‚
    model.add(Dense(512, activation='selu', kernel_regularizer=l2(0.01)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))

    model.add(Dense(256, activation='selu', kernel_regularizer=l2(0.01)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))

    # è¾“å‡ºå±‚
    model.add(Dense(num_classes, activation="softmax"))

    # # ç¼–è¯‘æ¨¡å‹
    # model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

    # # æ‰“å°æ¨¡å‹æ‘˜è¦
    # model.summary()

    # åŠ è½½æœ¬åœ°æƒé‡æ–‡ä»¶
    if os.path.exists(WEIGHTS_FILE_PATH):
        model.load_weights(WEIGHTS_FILE_PATH)
        logger.info(f"å·²åŠ è½½æƒé‡æ–‡ä»¶ï¼š{WEIGHTS_FILE_PATH}")
    else:
        logger.error(f"æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ï¼š{WEIGHTS_FILE_PATH}")
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ï¼š{WEIGHTS_FILE_PATH}")

    return model

```

+ CNN_GRAY_size48.py
+ CNN_RGB_size96.py

+ CNN_RGB_size224.py
+ Emotion_EffectiveV2.py

å¦‚æœéœ€è¦ä½¿ç”¨ï¼Œè¯·æ›¿æ¢åŸ`Emotion.py`çš„å†…å®¹



### 3ã€ç›´æ¥ä½¿ç”¨

```bash
git clone https://gitee.com/daetz_0/emotion_detection_client_server.git
# git clone https://github.com/daetz-coder/emotion_detection_client_server.git
cd emotion_detection_client_server
pip install -r requirements.txt
python main.py
# use the "q" exit
```





## äº”ã€å‚è€ƒé“¾æ¥



+ [GitHub - opencv/opencv: Open Source Computer Vision Library](https://github.com/opencv/opencv)

+ [GitHub - serengil/deepface: A Lightweight Face Recognition and Facial Attribute Analysis](https://github.com/serengil/deepface)
+ [Facial Emotion Recognition (kaggle.com)](https://www.kaggle.com/code/gauravsharma99/facial-emotion-recognition/notebook)
+ [Facial Expression Recognition(FER)Challenge (kaggle.com)](https://www.kaggle.com/datasets/ashishpatel26/facial-expression-recognitionferchallenge)

+ [ğŸ“ˆ EfficientNetV2 ğŸ˜ƒğŸ’¡ğŸ“Š Emotion Recognition ğŸ¤– (kaggle.com)](https://www.kaggle.com/code/guanlintao/efficientnetv2-emotion-recognition)
+ [Facial Emotion Recognition Image Dataset (kaggle.com)](https://www.kaggle.com/datasets/sujaykapadnis/emotion-recognition-dataset)